# Unlearning-LLM-papers
## Survey:
| Paper Title | Year | Venue | Code |
| ------------- | :-------------: | :-------------: | :-------------: |
[Digital forgetting in large language models: A survey of unlearning methods](https://link.springer.com/article/10.1007/s10462-024-11078-6) | 2025 | Artificial Intelligence Review | \ |
[Rethinking machine unlearning for large language models](https://www.nature.com/articles/s42256-025-00985-0) | 2025 | Nature Machine Intelligence | \
[A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models](https://arxiv.org/html/2503.01854v1) | 2025 | arxiv | \ |
[Deciphering the Impact of Pretraining Data on Large Language Models through Machine Unlearning](https://aclanthology.org/2024.findings-acl.559/) | 2024 | Findings of the Association for Computational Linguistics: ACL | \
[Machine Unlearning for Traditional Models and Large Language Models: A Short Survey](https://arxiv.org/abs/2404.01206) | 2024 | arxiv | \ |
[Machine Unlearning in Generative AI: A Survey](https://arxiv.org/abs/2407.20516) | 2024 | arxiv | \ |
[Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges](https://arxiv.org/abs/2311.15766) | 2023 | arxiv | \


## Benchmark:
| Paper Title | Year | Venue | Code |
| ------------- | :-------------: | :-------------: | :-------------: |
[Towards Effective Evaluations and Comparisons for LLM Unlearning Methods](https://openreview.net/forum?id=wUtCieKuQU) | 2025 | ICLR | [code](https://github.com/tmlr-group/Unlearning-with-Control) |
[The WMDP Benchmark: Measuring and Reducing Malicious Use with Unlearning](https://proceedings.mlr.press/v235/li24bc.html) | 2024 | ICML | [code](https://github.com/centerforaisafety/wmdp) 
[RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models](https://openreview.net/forum?id=wOmtZ5FgMH#discussion) | 2024 | NeurIPS Datasets and Benchmarks | \ |
[To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models](https://aclanthology.org/2024.findings-emnlp.82/) | 2024 |EMNLP | [code](https://github.com/zjunlp/KnowUnDo?utm_source=catalyzex.com)
Multimodal: | \ | \ | \ |
[Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench](https://arxiv.org/abs/2410.22108) | 2025 | NAACL | [code](https://github.com/franciscoliu/MLLMU-Bench?tab=readme-ov-file)
[PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models](https://arxiv.org/abs/2503.12545) | 2025 | arxiv | \ |
[Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation](https://openreview.net/forum?id=YcnjgKbZQS) | 2024 | TMLR | \ |

## LLM:
| Paper Title | Year | Venue | Code |
| ------------- | :-------------: | :-------------: | :-------------: |
[Large Scale Knowledge Washing](https://arxiv.org/abs/2405.16720) | 2025 | ICLR | [code](https://github.com/wangyu-ustc/largescalewashing)
[To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models](https://proceedings.mlr.press/v235/barbulescu24a.html) | 2024 | ICML | 
[In-Context Unlearning: Language Models as Few-Shot Unlearners](https://proceedings.mlr.press/v235/pawelczyk24a.html) | 2024 | ICML | \
[Large Language Model Unlearning via Embedding-Corrupted Prompts](https://proceedings.neurips.cc/paper_files/paper/2024/hash/d6359156e0e30b1caa116a4306b12688-Abstract-Conference.html) | 2024 | NIPS | [code](https://github.com/chrisliu298/llm-unlearn-eco)
[Large Language Model Unlearning](https://proceedings.neurips.cc/paper_files/paper/2024/hash/be52acf6bccf4a8c0a90fe2f5cfcead3-Abstract-Conference.html) | 2024 | NIPS | \
[To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models](https://aclanthology.org/2024.findings-emnlp.82/) | 2024 |EMNLP | [code](https://github.com/zjunlp/KnowUnDo?utm_source=catalyzex.com)
[Dissecting Fine-Tuning Unlearning in Large Language Models](https://aclanthology.org/2024.emnlp-main.228/) | 2024 | EMNLP | [code](https://github.com/yihuaihong/Dissecting-FT-Unlearning)
[Towards Safer Large Language Models through Machine Unlearning](https://aclanthology.org/2024.findings-acl.107/) | 2024 | Findings of the Association for Computational Linguistics: ACL | \
[Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning](https://openreview.net/forum?id=MXLBXjQkmb#discussion) | 2024 | COLM | \
[TOFU: A Task of Fictitious Unlearning for LLMs](https://openreview.net/forum?id=P8seBluN3c) | 2024 | NeurIPS 2024 Workshop Red Teaming GenAI | \
[Guardrail Baselines for Unlearning in LLMs](https://arxiv.org/abs/2403.03329) | 2024 | arxiv | \
[RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models](https://arxiv.org/abs/2406.01983) | 2024 | arxiv | \
[Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://aclanthology.org/2023.emnlp-main.738/) | 2023 | EMNLP | \
[Whoâ€™s Harry Potter? Approximate Unlearning for LLMs](https://openreview.net/forum?id=PDct7vrcvT) | 2023 | arxiv | \


## Multimodal LLM:
| Paper Title | Year | Venue | Code |
| ------------- | :-------------: | :-------------: | :-------------: |
[MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models](https://arxiv.org/abs/2502.11051) | 2025 | arxiv | \
[Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models](https://arxiv.org/abs/2502.15910) | 2025 | arxiv | \
[SafeEraser: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning](https://arxiv.org/abs/2502.12520) | 2025 |arxiv | \
[EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models](https://aclanthology.org/2024.emnlp-main.67/) | 2024 | EMNLP | \
[Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2024/hash/3e53d82a1113e3d240059a9195668edc-Abstract-Conference.html) | 2024 | NIPS | \ 

## Medical LLM:
| Paper Title | Year | Venue | Code |
| ------------- | :-------------: | :-------------: | :-------------: |
[Medical large language models are vulnerable to data-poisoning attacks](https://www.nature.com/articles/s41591-024-03445-1) | 2024 | nature medicine | \
